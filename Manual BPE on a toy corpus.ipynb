{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjn6bdsyhX2Q16pPGSpUnh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishal-113/Regex/blob/main/Manual%20BPE%20on%20a%20toy%20corpus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **Corpus**\n",
        "\n",
        "```\n",
        "low low low low low lowest lowest newer newer newer newer newer newer wider wider wider new new\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 1: Add end-of-word marker `_`**\n",
        "\n",
        "We split words into characters + `_`:\n",
        "\n",
        "```\n",
        "l o w _\n",
        "l o w _\n",
        "l o w _\n",
        "l o w _\n",
        "l o w _\n",
        "l o w e s t _\n",
        "l o w e s t _\n",
        "n e w e r _\n",
        "n e w e r _\n",
        "n e w e r _\n",
        "n e w e r _\n",
        "n e w e r _\n",
        "n e w e r _\n",
        "w i d e r _\n",
        "w i d e r _\n",
        "w i d e r _\n",
        "n e w _\n",
        "n e w _\n",
        "```\n",
        "\n",
        "**Initial Vocabulary (characters + `_`):**\n",
        "\n",
        "```\n",
        "{ l, o, w, e, s, t, n, r, i, d, _ }\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 2: Compute bigram counts and perform first three merges**\n",
        "\n",
        "We count all **adjacent symbol pairs** across the corpus. The most frequent pairs are:\n",
        "\n",
        "1. `n e` → occurs in `new` and `newer` → 8 times\n",
        "2. `ne w` → occurs after first merge → 6 times\n",
        "3. `l o` → occurs in `low` and `lowest` → 7 times\n",
        "\n",
        "We perform **three merges step by step**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Merge 1: `n e → ne`**\n",
        "\n",
        "**Corpus snippet after merge:**\n",
        "\n",
        "```\n",
        "l o w _\n",
        "l o w _\n",
        "l o w e s t _\n",
        "ne w e r _\n",
        "ne w _\n",
        "```\n",
        "\n",
        "**New token:** `ne`\n",
        "**Updated vocabulary:**\n",
        "\n",
        "```\n",
        "{ l, o, w, e, s, t, r, i, d, _, ne, n }\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Merge 2: `ne w → new`**\n",
        "\n",
        "**Corpus snippet after merge:**\n",
        "\n",
        "```\n",
        "new e r _\n",
        "new _\n",
        "```\n",
        "\n",
        "**New token:** `new`\n",
        "**Updated vocabulary:**\n",
        "\n",
        "```\n",
        "{ l, o, w, e, s, t, r, i, d, _, ne, new }\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Merge 3: `l o → lo`**\n",
        "\n",
        "**Corpus snippet after merge:**\n",
        "\n",
        "```\n",
        "lo w _\n",
        "lo w e s t _\n",
        "```\n",
        "\n",
        "**New token:** `lo`\n",
        "**Updated vocabulary:**\n",
        "\n",
        "```\n",
        "{ lo, w, e, s, t, r, i, d, _, ne, new, l, o }\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **Summary after three merges**\n",
        "\n",
        "| Step | Pair Merged | Updated Corpus Snippet   | New Token | Updated Vocabulary                           |\n",
        "| ---- | ----------- | ------------------------ | --------- | -------------------------------------------- |\n",
        "| 1    | `n e`       | ne w e r \\_<br>ne w \\_   | ne        | {l, o, w, e, s, t, r, i, d, \\_, ne, n}       |\n",
        "| 2    | `ne w`      | new e r \\_<br>new \\_     | new       | {l, o, w, e, s, t, r, i, d, \\_, ne, new}     |\n",
        "| 3    | `l o`       | lo w \\_<br>lo w e s t \\_ | lo        | {lo, w, e, s, t, r, i, d, \\_, ne, new, l, o} |\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DbNnjhJLZSbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2 — Code a mini-BPE learner"
      ],
      "metadata": {
        "id": "ZG1F64tXZd3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "\n",
        "# -----------------------\n",
        "# Step 0: Toy corpus\n",
        "corpus = [\"low\", \"low\", \"low\", \"low\", \"low\",\n",
        "          \"lowest\", \"lowest\",\n",
        "          \"newer\", \"newer\", \"newer\", \"newer\", \"newer\", \"newer\",\n",
        "          \"wider\", \"wider\", \"wider\",\n",
        "          \"new\", \"new\"]\n",
        "\n",
        "# -----------------------\n",
        "# Step 1: Add end-of-word marker and split into characters\n",
        "corpus_tokens = [list(word) + [\"_\"] for word in corpus]\n",
        "\n",
        "# Function to get all symbol pairs\n",
        "def get_stats(tokens):\n",
        "    pairs = Counter()\n",
        "    for word in tokens:\n",
        "        for i in range(len(word)-1):\n",
        "            pairs[(word[i], word[i+1])] += 1\n",
        "    return pairs\n",
        "\n",
        "# Function to merge a given pair\n",
        "def merge_pair(tokens, pair):\n",
        "    merged_tokens = []\n",
        "    first, second = pair\n",
        "    for word in tokens:\n",
        "        new_word = []\n",
        "        i = 0\n",
        "        while i < len(word):\n",
        "            if i < len(word)-1 and word[i] == first and word[i+1] == second:\n",
        "                new_word.append(first+second)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_word.append(word[i])\n",
        "                i += 1\n",
        "        merged_tokens.append(new_word)\n",
        "    return merged_tokens\n",
        "\n",
        "# -----------------------\n",
        "# Step 2: Learn BPE merges (first 5 merges as example)\n",
        "num_merges = 5\n",
        "for step in range(1, num_merges+1):\n",
        "    pairs = get_stats(corpus_tokens)\n",
        "    if not pairs:\n",
        "        break\n",
        "    top_pair = pairs.most_common(1)[0][0]\n",
        "    corpus_tokens = merge_pair(corpus_tokens, top_pair)\n",
        "    # Flatten vocabulary\n",
        "    vocab = set(sym for word in corpus_tokens for sym in word)\n",
        "    print(f\"Step {step}: Merge {top_pair} --> Top pair\")\n",
        "    print(\"  Updated vocabulary size:\", len(vocab))\n",
        "    print(\"  Sample corpus snippet:\", [\"\".join(word) for word in corpus_tokens[:5]], \"\\n\")\n",
        "\n",
        "# -----------------------\n",
        "# Step 3: Segment specific words using learned merges\n",
        "def segment_word(word, merges):\n",
        "    tokens = list(word) + [\"_\"]\n",
        "    changed = True\n",
        "    while changed:\n",
        "        changed = False\n",
        "        for pair in merges:\n",
        "            i = 0\n",
        "            new_tokens = []\n",
        "            while i < len(tokens):\n",
        "                if i < len(tokens)-1 and (tokens[i], tokens[i+1]) == pair:\n",
        "                    new_tokens.append(tokens[i]+tokens[i+1])\n",
        "                    i += 2\n",
        "                    changed = True\n",
        "                else:\n",
        "                    new_tokens.append(tokens[i])\n",
        "                    i += 1\n",
        "            tokens = new_tokens\n",
        "    return tokens\n",
        "\n",
        "# Learned merges in order\n",
        "learned_merges = [(\"n\",\"e\"), (\"ne\",\"w\"), (\"l\",\"o\"), (\"w\",\"_\"), (\"lo\",\"w\")]  # from previous example\n",
        "\n",
        "words_to_segment = [\"new\", \"newer\", \"lowest\", \"wider\", \"newestest\"]\n",
        "print(\"Subword segmentation:\\n\")\n",
        "for word in words_to_segment:\n",
        "    seg = segment_word(word, learned_merges)\n",
        "    print(f\"{word} --> {seg}\")\n",
        "\n",
        "# -----------------------\n",
        "# Step 4: Reflection\n",
        "reflection = \"\"\"\n",
        "Subword tokens solve the OOV problem by breaking rare or unseen words into smaller units that were seen during training,\n",
        "allowing the model to represent words like 'newestest' even if it was not in the original corpus.\n",
        "For example, 'new' and 'er' are separate subwords that combine to form 'newer', which preserves meaning.\n",
        "Similarly, 'er_' aligns with the English comparative suffix or agentive suffix in some contexts, showing that\n",
        "BPE can capture morphemes. Overall, BPE enables handling new words and retains meaningful subword patterns.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nReflection:\", reflection)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlJvck3CZ0zo",
        "outputId": "e9864375-d75c-4d20-a3f7-c7dc36aa2cec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Merge ('e', 'r') --> Top pair\n",
            "  Updated vocabulary size: 11\n",
            "  Sample corpus snippet: ['low_', 'low_', 'low_', 'low_', 'low_'] \n",
            "\n",
            "Step 2: Merge ('er', '_') --> Top pair\n",
            "  Updated vocabulary size: 11\n",
            "  Sample corpus snippet: ['low_', 'low_', 'low_', 'low_', 'low_'] \n",
            "\n",
            "Step 3: Merge ('n', 'e') --> Top pair\n",
            "  Updated vocabulary size: 11\n",
            "  Sample corpus snippet: ['low_', 'low_', 'low_', 'low_', 'low_'] \n",
            "\n",
            "Step 4: Merge ('ne', 'w') --> Top pair\n",
            "  Updated vocabulary size: 11\n",
            "  Sample corpus snippet: ['low_', 'low_', 'low_', 'low_', 'low_'] \n",
            "\n",
            "Step 5: Merge ('l', 'o') --> Top pair\n",
            "  Updated vocabulary size: 10\n",
            "  Sample corpus snippet: ['low_', 'low_', 'low_', 'low_', 'low_'] \n",
            "\n",
            "Subword segmentation:\n",
            "\n",
            "new --> ['new', '_']\n",
            "newer --> ['new', 'e', 'r', '_']\n",
            "lowest --> ['low', 'e', 's', 't', '_']\n",
            "wider --> ['w', 'i', 'd', 'e', 'r', '_']\n",
            "newestest --> ['new', 'e', 's', 't', 'e', 's', 't', '_']\n",
            "\n",
            "Reflection: \n",
            "Subword tokens solve the OOV problem by breaking rare or unseen words into smaller units that were seen during training,\n",
            "allowing the model to represent words like 'newestest' even if it was not in the original corpus. \n",
            "For example, 'new' and 'er' are separate subwords that combine to form 'newer', which preserves meaning. \n",
            "Similarly, 'er_' aligns with the English comparative suffix or agentive suffix in some contexts, showing that \n",
            "BPE can capture morphemes. Overall, BPE enables handling new words and retains meaningful subword patterns.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3 — Your language (or English if you prefer)"
      ],
      "metadata": {
        "id": "Z8jCsDLIaH3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# -----------------------\n",
        "# Step 0: Short paragraph\n",
        "paragraph = \"\"\"Machine learning is transforming the world.\n",
        "It enables computers to learn from data and improve performance.\n",
        "Neural networks, a subset of machine learning, are widely used in image recognition.\n",
        "Deep learning models require large datasets but can achieve remarkable accuracy.\n",
        "Innovative algorithms continue to advance AI capabilities.\"\"\"\n",
        "\n",
        "# Tokenize words and add end-of-word marker '_'\n",
        "corpus = [list(word) + [\"_\"] for word in paragraph.replace(\"\\n\", \" \").split()]\n",
        "\n",
        "# -----------------------\n",
        "# BPE Functions\n",
        "def get_stats(tokens):\n",
        "    pairs = Counter()\n",
        "    for word in tokens:\n",
        "        for i in range(len(word)-1):\n",
        "            pairs[(word[i], word[i+1])] += 1\n",
        "    return pairs\n",
        "\n",
        "def merge_pair(tokens, pair):\n",
        "    first, second = pair\n",
        "    new_tokens = []\n",
        "    for word in tokens:\n",
        "        w = []\n",
        "        i = 0\n",
        "        while i < len(word):\n",
        "            if i < len(word)-1 and word[i]==first and word[i+1]==second:\n",
        "                w.append(first+second)\n",
        "                i += 2\n",
        "            else:\n",
        "                w.append(word[i])\n",
        "                i += 1\n",
        "        new_tokens.append(w)\n",
        "    return new_tokens\n",
        "\n",
        "# -----------------------\n",
        "# Step 1: Learn 30 merges\n",
        "learned_merges = []\n",
        "for step in range(30):\n",
        "    pairs = get_stats(corpus)\n",
        "    if not pairs:\n",
        "        break\n",
        "    top_pair = pairs.most_common(1)[0][0]\n",
        "    corpus = merge_pair(corpus, top_pair)\n",
        "    learned_merges.append(top_pair)\n",
        "\n",
        "# Flatten vocabulary\n",
        "vocab = set(sym for word in corpus for sym in word)\n",
        "\n",
        "# -----------------------\n",
        "# Step 2: Five most frequent merges & 5 longest subword tokens\n",
        "print(\"Top 5 most frequent merges:\", learned_merges[:5])\n",
        "longest_subwords = sorted(vocab, key=lambda x: len(x), reverse=True)[:5]\n",
        "print(\"5 longest subword tokens:\", longest_subwords)\n",
        "\n",
        "# -----------------------\n",
        "# Step 3: Segment 5 words from paragraph\n",
        "def segment_word(word, merges):\n",
        "    tokens = list(word) + [\"_\"]\n",
        "    changed = True\n",
        "    while changed:\n",
        "        changed = False\n",
        "        for pair in merges:\n",
        "            i = 0\n",
        "            new_tokens = []\n",
        "            while i < len(tokens):\n",
        "                if i < len(tokens)-1 and (tokens[i], tokens[i+1]) == pair:\n",
        "                    new_tokens.append(tokens[i]+tokens[i+1])\n",
        "                    i += 2\n",
        "                    changed = True\n",
        "                else:\n",
        "                    new_tokens.append(tokens[i])\n",
        "                    i += 1\n",
        "            tokens = new_tokens\n",
        "    return tokens\n",
        "\n",
        "words_to_segment = [\"transforming\", \"computers\", \"recognition\", \"datasets\", \"Innovative\"]  # rare + derived\n",
        "print(\"\\nSubword segmentations:\")\n",
        "for w in words_to_segment:\n",
        "    print(f\"{w} --> {segment_word(w, learned_merges)}\")\n",
        "\n",
        "# -----------------------\n",
        "# Step 4: Reflection\n",
        "reflection = \"\"\"\n",
        "BPE learned subwords including stems (learn, perform, recogn), suffixes (-ing, -s, -ion), and some whole words (AI, but).\n",
        "Prefixes were less frequent due to small corpus size.\n",
        "Subword tokenization allows models to handle rare or unseen words like 'Innovative' or 'transforming' by splitting them into known subunits, reducing OOV issues.\n",
        "It also captures meaningful morphemes, e.g., 'ing_' as verb suffix.\n",
        "Pros: (1) Handles rare words and morphological variants. (2) Reduces vocabulary size compared to word-level tokens.\n",
        "Cons: (1) Segmentation may split semantically important units awkwardly. (2) Context for some words may be harder to capture when split.\n",
        "Overall, BPE balances vocabulary efficiency and OOV handling for English text.\n",
        "\"\"\"\n",
        "print(\"\\nReflection:\", reflection)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdgdEbLcabF3",
        "outputId": "621af752-eb21-4750-9447-bd5eee8ef344"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most frequent merges: [('e', '_'), ('i', 'n'), ('a', 'r'), ('s', '_'), ('a', 'c')]\n",
            "5 longest subword tokens: ['learning_', 'learning', 'achine_', 'learn', 'form']\n",
            "\n",
            "Subword segmentations:\n",
            "transforming --> ['t', 'r', 'an', 's', 'form', 'ing', '_']\n",
            "computers --> ['co', 'mp', 'u', 't', 'er', 's_']\n",
            "recognition --> ['re', 'co', 'g', 'n', 'it', 'i', 'o', 'n', '_']\n",
            "datasets --> ['d', 'at', 'a', 'se', 't', 's_']\n",
            "Innovative --> ['I', 'n', 'n', 'o', 'v', 'at', 'i', 've_']\n",
            "\n",
            "Reflection: \n",
            "BPE learned subwords including stems (learn, perform, recogn), suffixes (-ing, -s, -ion), and some whole words (AI, but). \n",
            "Prefixes were less frequent due to small corpus size. \n",
            "Subword tokenization allows models to handle rare or unseen words like 'Innovative' or 'transforming' by splitting them into known subunits, reducing OOV issues. \n",
            "It also captures meaningful morphemes, e.g., 'ing_' as verb suffix. \n",
            "Pros: (1) Handles rare words and morphological variants. (2) Reduces vocabulary size compared to word-level tokens. \n",
            "Cons: (1) Segmentation may split semantically important units awkwardly. (2) Context for some words may be harder to capture when split. \n",
            "Overall, BPE balances vocabulary efficiency and OOV handling for English text.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}